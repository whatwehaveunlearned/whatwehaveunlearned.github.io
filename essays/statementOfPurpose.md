---
layout: essay
type: essay
title: Statement of Purpose
date: 2019-04-01
labels:
  - PhD Portfolio
---

## Background and Research Interests

I have always had a passion to understand complex real time systems. From my background in intelligent robotics I have gravitated towards other real time, model based, challenging systems such as visual analytic systems and their combination with humans and collaborative work. In order to understand how to provide better tools and techniques that help groups of people make sense together of the complex world we all live in.

In 1755 the French philosopher Denis Diderot made the following prophecy which two and a half centuries later has become mostly true: "As long as the centuries continue to unfold, the number of books will grow continually and one can predict that a time will come when it will be almost as difficult to learn anything from books as from the direct study of the whole universe. It will be almost as convenient to search for some bit of truth concealed in nature as it will be to find it hidden away in an immense multitude of bound volumes"

Machine Learning techniques are in the rise but understanding the outcomes of complex machine learning algorithms in order to make better decisions still remains a challenge. Interacting with this models and visualizations is the best way to gain insight into the nuances of the data that are lost in the representations but expert knowledge is required from the users of this systems to be able to iterate over the model and algorithms. 

Interpreting the results of Visual Analytics systems using black box machine learning techniques such as dimension reduction and clustering is not a simple task. It usually requires interactive enhancements that allow to change between different projections of the data and clusters in order to better interpret the results.

The use of external visual elements such as sliders is the most common approach in the literature but users suffer from cognitive load trying to map the visualizations, the interactive controls and the underlying algorithms. An interesting approach in this direction couples user interaction that directly affects the underlying model. The main challenge in the design of this type of systems is the semantic translation of the user interactions into actionable actions that update the underlying model and algorithms.
 
Past research in explainability and trust shows that explaining the models and the results to the users can improve their understanding and enhance the analysis. These challenges imply that  easily interpretable Visual Analytic systems would benefit from clearly "understanding" user intent while users will also benefit from interpreting the decisions taken by the system based on their interactions. Introducing concepts of explainable AI into Visual Analytic systems would not only help analyst to interpret the system outcomes but also help the Visual Analytic system better interpret the user interactions. By interpreting the semantic meaning of user interactions, each of these algorithms can better enable exploratory data analysis.

 For example, an analyst may wish to know what model parameters are necessary to create a cluster from different observations. By manipulating the projection to form such a cluster, the dimension reduction and clustering algorithms can be trained to learn such model and to update the entire projection in response to those new parameters. Even if the analyst does not completely understand the underlying model.  This new projection may create additional clusters from other data points in addition to the user created cluster, providing additional insights into the dataset. But within this process the analyst is able to understand the interpretation of the previous interaction. 

For example, the user can adjust the interaction if it realizes it was mistakenly interpreted by the system which creates a back and forth dialogue between the system and the analyst. This dialogue brings us closer to real computing collaborators that help throughout the exploration process responding to the analysts interactions and feedback.

My research interest lies in between these concepts in order to bridge the gap and simplify the understanding of machine learning outcomes, creating better visual analytical tools that behave more like active collaborators than rather passive systems improving the interpretation and analysis of the results.

## Progress

In the past 3 years I have been developing my skills in Machine Learning, Data Scienca and Visualizations. I have developed different tools that allow to better understand text content. Extracting topics, words, parameters authors and other statistic from pdfs. I have developed tools to provide dynamic brainstorming aids in collaboration environments. I have also worked on visualization tools that automatically select the “best” visualizations based on correlations and other parameters. I have also developed interpretable multidimensional visualizations based on forces which allow users to learn about the visualization in a constructive step-by-step process providing better understanding on the data ratios in different dimensions.

I have been working with NSF grants to develop tools to visualize the International Research Networks creating integrated dashboards that directly answer the most important questions for different level stakeholders and also to create Visualization systems that directly create visualizations from speech commands. All of these work and other not fructiferous attempts has allowed me to develop the research foundation and understanding

In the past year, with funding from NSA, I have been implementing the skeleton of a system that is able to process document collections and extract topics from them.

## Goals

In the next year I plan to introduce semantic interactions in combination with explainable AI techniques within the system that allow to play with the underlying algorithms without understanding their nuances and parameters. I will start working on monitoring metrics and performing some user studies with novice users in order to understand specific ways in which explainable AI and semantic interaction can be effectively combined to improve interpretation not only of the results but as a facilitator to the system to also better interpret the interaction providing a back and forth “dialogue” between systems and users. 

 
