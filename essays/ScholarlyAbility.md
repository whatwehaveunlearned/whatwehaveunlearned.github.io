---
layout: essay
type: essay
title: Evidence of Scholarly Ability
date: 2019-04-05
labels:
  - PhD Portfolio
---

This page represents evidence of Alberto Gonzalez Scholarly Abilities in Computer Science as required for the ICS PhD Degree.

## Lit Review

[Lit Review: High Dimensional Visual Analytics with Dimensionality Reduction and Clustering: a Literary Review.](https://drive.google.com/file/d/17MqJ-oy2thOljtXsnjr_PeE8HsowM6MW/view?usp=sharing)

## Publications

### Sage River Disaster Information (SageRDI): Demonstrating Application Data Sharing In SAGE2
kobayashi D., Matthew R., Gonzalez Martinez A., Kirshenbaum K., Seto-Mook T., Leigh J., Haga J.

2018 ACM International Conference

The Scalable Amplified Group Environment (SAGE2) is an open-source, web-based middleware for driving tiled display walls. SAGE2 allows running multiple applications at once within its workspace. In large display walls, users tend to collaborate using multiple applications in the same space for simultaneous interaction and review. Unfortunately, many of these applications were created independently by different developers and were never intended to interoperate which greatly limits their potential reusability. SAGE2 developers face system limitations where applications are data segregated and cannot easily communicate with others. To counter this problem, we developed the SAGE2 data sharing components. We describe the Sage River Disaster Information (SageRDI) application and the SAGE2 architectural implementations necessary for its operation. SageRDI enables river.go.jp, an existing website that provides water sensor data for Japan, to interact with other SAGE2 applications without modifying the website's server, hosted files, nor any of the default SAGE2 applications.

My contribution to this paper is around 10%. Helping with the study and the theoretical framework to ground the results to.  

[Paper](https://www.researchgate.net/publication/329108066_Sage_River_Disaster_Information_SageRDI_Demonstrating_Application_Data_Sharing_In_SAGE2)

### Multimodal Coreference Resolution for Exploratory Data Visualization Dialogue: Context-Based Annotation and Gesture Identification
Kumar A., Aurisano J., Di Eugenio B., Johnson A., Alsaiari A., Flowers N., Gonzalez Martinez A., Leigh J.

SEMDIAL 2017 (SaarDial) Workshop on the Semantics and Pragmatics of Dialogue

The goals of our work are twofold:  gaininsight  into  how  humans  interact  withcomplex data and visualizations thereof inorder  to  make  discoveries;  and  use  ourfindings to develop a dialogue system forexploring  data  visualizations.   Crucial  toboth goals is understanding and modelingof  multimodal  referential  expressions,  inparticular  those  that  include  deictic  ges-tures.  In this paper, we discuss how con-text information affects the interpretationof  requests  and  their  attendant  referringexpressions in our data.    To this end, wehave  annotated  our  multimodal  dialoguecorpus for context and both utterance andgesture  information;   we  have  analyzedwhether a gesture co-occurs with a specificrequest or with the context surrounding therequest;  we have started addressing mul-timodal  co-reference  resolution  by  usingKinect to detect deictic gestures;  and wehave  started  identifying  themes  found  inthe annotated context,  especially in whatfollows the request

This work is constructing on previuos work and systems we developed in the previous years. My contributions to this paper is around 10% setting up tools and pipeline steps and other the subsystems involved in the study. 

[Paper](https://www.researchgate.net/publication/319128704_Multimodal_Coreference_Resolution_for_Exploratory_Data_Visualization_Dialogue_Context-Based_Annotation_and_Gesture_Identification)

### Big Data and Analysis of Data Transfers for International Research Networks Using NetSage

Gonzalez Martinez A., Leigh J., Peisert S., Tierney B., Balas E., Radulovic P, Schopf JM.

2017 IEEE International Congress on Big Data (BigData Congress)

Modern   science   is   increasingly   data-driven   and collaborative  in  nature.  Many  scientific  disciplines,including genomics,  high-energy  physics,  astronomy,and  atmospheric science,  produce  petabytes  of  data  that  must  be  sharedwithcollaboratorsall   over   the   world.   The   National   Science Foundation-supported International Research Network Connection  (IRNC)  links  have  been  essential  to enabling  this collaboration,  but  as  data  sharing  has  increased,  so  has  the amountof  information  being  collected  to  understand  network performance. New  capabilities  to  measure  and  analyze  the performance of international wide-area networks are essential to  ensure  end-users  are  able  to  take  full  advantage  of  such infrastructure for their big data applications.NetSage is a project to develop a unified, open, privacy-aware network measurement, and visualization service to address the needs  of  monitoring  today’s  high-speed  international  research networks.  NetSage  collects  data  on  both  backbone  links  and exchange points, which can be as much as1Tb per month.This puts a significant strain on hardware,not only in terms storage needs  to  hold  multi-year  historical  data,  but  also  in  terms  of processor and memory needs to analyze the data to understand network behaviors. 

My contributions to this paper is around 90%, the definition, research. design, implementation and iterations on the different visualization tools used in the system as well as writing the paper.

[Paper](https://www.researchgate.net/publication/319637604_Big_Data_and_Analysis_of_Data_Transfers_for_International_Research_Networks_Using_NetSage)

### The Destiny-class CyberCANOE - a surround screen stereoscopic, cyber-enabled collaboration analysis navigation and observation environment

Kawano, N., Theriot, R., Lam, J., Wu, E., Guagliardo, A., Kobayashi, D., Gonzalez, A., Uchida, K., Leigh, J.

IS&T Electronic Imaging 2017, Engineering Reality of Virtual Reality, 2017. January 2017

The Destiny-class CyberCANOE is a hybrid-reality environment that provides 20/20 visual acuity in a 13-foot-wide, 320-degree cylindrical structure comprised of tiled passive stereo-capable organic light emitting diode (OLED) displays. Hybrid-reality systems such as Destiny, CAVE2, WAVE and the TourCAVE combine surround-screen virtual reality environments with ultra-high-resolution digital project-rooms. They are intended as collaborative environments that enable multiple users to work minimally encumbered, and hence comfortably, for long periods of time in rooms surrounded by data in the form of visualizations that benefit from being displayed at resolutions matching visual acuity and in stereoscopic 3D. Destiny is unique in that: it is the first hybrid-reality system to use OLED displays; it uses a real-time software-based approach rather than a physical optical approach for minimizing stereoscopic crosstalk when images are viewed severely off-axis on polarized stereoscopic displays; and it used Microsoft’s HoloLens augmented reality display to prototype its design and aid in its construction. This paper will describe Destiny’s design and implementation - in particular the technique for software-based crosstalk mitigation. Lastly it will describe how the HoloLens helped validate Destiny’s design as well as train the construction team in its assembly.

My contribution of this paper was around 30% helping with some of the resarch sections of the study and writing some of the sections. 

[Paper](https://www.researchgate.net/publication/319861599_The_Destiny-class_CyberCANOE_-_a_surround_screen_stereoscopic_cyber-enabled_collaboration_analysis_navigation_and_observation_environment)

### Articulate2: Toward a Conversational Interface for Visual Data Exploration
Kumar, A., Aurisano, J., Di Eugenio, B., Johnson, A., Gonzalez, A., Leigh, J.

Poster Presented at the Information Visualization Conference at IEEE VisWeek in Baltimore. October 2016

InfoVis novices struggle with visualization construction. Even with the aid of visualization software, such users may face challenges when translating their questions into appropriate visual encodings, or interactively refining the representation to achieve a desired result. A ‘conversational interface’ which maintains a dialog with the user through natural language and gestures, could allow users to engage in repeated cycles of visualization generation and modification, asking questions directly through speech. In this poster we present a prototype conversational visual data analysis system. Our prototype was developed from a corpus consisting in 15-subjects engaging in exploratory data visualization with a simulated conversational interface. It features 1) speech to visualization pipeline, 2) classification system to divide utterances into major types, 3) history manager and knowledge-base.

My contributions to this paper was about 50% of the total work. I defined and helped envision and develop the structure of the architecture to perform live visual queries. I wrote sections of this paper. 

[Paper](https://www.researchgate.net/publication/311559539_Towards_a_Dialogue_System_that_Supports_Rich_Visualizations_of_Data)

### Towards a Dialogue System that Supports Rich Visualizations of Data
J. Aurisano, A. Kumar, A. Gonzalez, J Leigh, B. DiEugenio, A. Johnson.

The 17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL 2016), Los Angeles, CA, September 2016

The goal of our research is to support full-fledged dialogue between a user and a system that transforms the user queries into visualizations. So far, we have collected a corpus where users explore data via visualizations; we have annotated the corpus for user intentions; and we have developed the core NL-to-visualization pipeline.

My contributions to this paper was around 70% of the total work. Tagging and analysing corpuses and helping to develop the coding mechanisms and the utterances that should be recorded. I did reviews and research on visualization task taxonomies to come up with the user interactions. I also worked researching and implementing some of the pieces of the NL pipeline.

[Paper](https://www.researchgate.net/publication/311559539_Towards_a_Dialogue_System_that_Supports_Rich_Visualizations_of_Data)

### NetSage: Open Privacy-Aware Network Measurement, Analysis and Visualization Service
Alberto Gonzalez, Jason Leigh, Sean Peisert, Brian Tierney, Andrew Lee, and Jennifer M. Schopf

TNC16 Conference 12-16 June 2016, Prague, Czech Republic, June 2016

NetSage is a project to develop a unified open, privacy-aware network measurement, and visualization service to address the needs of today’s international networks. Modern science is increasingly data-driven and collaborative in nature, producing petabytes of data that can be shared by tens to thousands of scientists all over the world. The National Science, Foundation-supported International Research Network Connections (IRNC) links, have been essential to performing these science experiments. Recent deployment of Science DMZs [Dart, E. et al., 2013], both in the US and other countries, is starting to raise expectations for data throughput performance for wide-area data transfers. New capabilities to measure and analyze the capacity of international wide-area networks are essential to ensure end-users are able to take full advantage of such infrastructure. NetSage will provide the network engineering community, both US domestic and international, with a suite of tools and services to more deeply understand: 1) the current traffic patterns across IRNC links, and anticipate growth trends for capacity-planning purposes; 2) the main sources and sinks of large, elephant flows to know where to focus outreach and training opportunities; and 3) the cause of packet losses in the links and how they impact end-to- end performance.

My contributions to this paper was around 90% of the total work. Research and implement the visualizations, interaction design and the full visualization dashboard of the tool. As well as writing the paper and the theoretic backgrounds.

[Paper](https://www.researchgate.net/publication/321373138_NETSAGE_OPEN_PRIVACY-AWARE_NETWORK_MEASUREMENT_ANALYSIS_AND_VISUALIZATION_SERVICE)

#### "Show Me Data" Observational Study of a Conversational Interface in Visual Data Exploration
Aurisano, J., Kumar, A., Gonzalez, A., Reda, K., Leigh, J., DiEugenio, B., Johnson, A.

IEEE Visualization 2015, Chicago, IL, Honorable Mention , October 2015

A natural language interface to visual data exploration would allow a user to directly specify questions through speech, allowing the user to focus on higher-order tasks, such as hypothesis generation and question formulation. However, visual data exploration involves repeated cycles of visualization construction and interaction, as well as reasoning across many visualizations generated over the course of an exploratory session. A ’conversational interface’, which maintains a dialog with the user through natural language and gestures, could support these complex tasks.We conducted an observational, exploratory study to observe the interaction between a subject and a remote data analysis expert (DAE) who assists the subject in an exploratory data analysis task. I wrote different sections of this paper.

[Paper](https://www.researchgate.net/publication/321372960_Show_me_data_Observational_study_of_a_conversational_interface_in_visual_data_exploration?_sg=NqbxI_EXi0mMSXHTmWTfe9y-ykQlDwaJs9HXkD2SNUwcRvXuncsJocb_wEf8P0EDpddp1f7oM6SMJHJxnhBPrERSucmuXLmo7u4mmv6N.5ohBprnI_Hah5Jtg5bX4w6t_MnPV5CQI4hti5BPfURKXCMTQfaXaDgtGuC-n2ObCY0-TZc7UwSRHvnyPkUySzg)

My contributions to this paper amount to 60% of the total work. Research in Machine-Human Dialogue Methods. Natural Language Processing techniques and tools that we could leverage to build the system. Defining the form and structure of the study. I also help with data analysis and interpretations of the results. I wrote different sections of this paper.

### Tell Me What do You See: Detecting Perceptually-Separable Visual Patterns Via Clustering of Image-Space Features in Visualizations.
K. Reda, A. Gonzalez, J. Leigh, M. Papka.

8th Annual Postdoctoral Research Symposium, Argonne National Laboratory, Argonne, IL, Robert G. Sachs award, 1st place, October 2015

Visualization helps users infer structures and relationships in the data by encoding information as visual features that can be processed by the human visual-perceptual system. However, users would typically need to expend significant effort to scan and analyze a large number of views before they can begin to recognize relationships in a visualization. We propose a technique to partially automate the process of analyzing visualizations. By deriving and analyzing image-space features from visualizations, we can detect perceptually-separable patterns in the information space. We summarize these patterns with a tree-based meta-visualization and present it to the user to aid exploration. We illustrate this technique with an example scenario involving the analysis of census data.

My contributions to this paper was 40% of the work to research visual analytic techniques to simplify the analyst analysis. Researching techniques on how to implement the analysis directly in the visual elements instead of the raw data as well as writing some of the sections. 

[Paper](https://www.researchgate.net/publication/308835777_Tell_me_what_do_you_see_Detecting_perceptually-separable_visual_patterns_via_clustering_of_image-space_features_in_visualizations)

